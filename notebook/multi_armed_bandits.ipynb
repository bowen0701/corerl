{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLAI - Chapter 2: Multi-armed Bandits\n",
    "\n",
    "## The 10-armed Testbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"Environment class for k-armed bandit.\"\"\"\n",
    "\n",
    "    def __init__(self, K):\n",
    "        # Simulate k means from standard normal N(0, 1).\n",
    "        self.K = K\n",
    "        self.reward_means = np.random.randn(self.K)\n",
    "        self.optim_action = np.argmax(self.reward_means)\n",
    "\n",
    "    def get_actions(self):\n",
    "        \"\"\"Get possible (fixed) actions.\"\"\"\n",
    "        return list(range(self.K))\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Step by action to get reward.\"\"\"\n",
    "        return np.random.randn() + self.reward_means[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBanditAgent:\n",
    "    \"\"\"Agent class for stationary multi-armed bandit.\"\"\"\n",
    "\n",
    "    def __init__(self, K, epsilon=0.1, optim_init_values=None):\n",
    "        self.K = K\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        if optim_init_values:\n",
    "            self.optim_init_values = optim_init_values\n",
    "        else:\n",
    "            self.optim_init_values = 0\n",
    "\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def init_action_values(self):\n",
    "        \"\"\"Initialize action values.\"\"\"\n",
    "        self.Q = [0 + self.optim_init_values] * self.K\n",
    "        self.N = [0] * self.K\n",
    "\n",
    "    def _explore(self, actions):\n",
    "        \"\"\"Random exploration.\"\"\"\n",
    "        np.random.shuffle(actions)\n",
    "        n = len(actions)\n",
    "        action = actions[np.random.randint(n)]\n",
    "        return action\n",
    "\n",
    "    def _exploit_and_explore(self, actions):\n",
    "        \"\"\"Exploit and explore by the epsilon-greedy strategy:\n",
    "          - Take exploratory moves in the p% of times. \n",
    "          - Take greedy moves in the (100-p)% of times.\n",
    "        where p% is epsilon. \n",
    "        If epsilon is zero, then use the greedy strategy.\n",
    "        \"\"\"\n",
    "        p = np.random.random()\n",
    "        if p > self.epsilon:\n",
    "            # Exploit by selecting the action with the greatest value and\n",
    "            # breaking ties randomly.\n",
    "            vals_actions = []\n",
    "            for a in actions:\n",
    "                v = self.Q[a]\n",
    "                vals_actions.append((v, a))\n",
    "            np.random.shuffle(vals_actions)\n",
    "            vals_actions.sort(key=lambda x: x[0], reverse=True)\n",
    "            action = vals_actions[0][1]\n",
    "        else:\n",
    "            # Explore by selecting action randomly.\n",
    "            action = self._explore(actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def select_action(self, env):\n",
    "        \"\"\"Select an action from possible actions.\"\"\"\n",
    "        # Get next actions from environment.\n",
    "        actions = env.get_actions()\n",
    "\n",
    "        # Exloit and explore by the epsilon-greedy strategy.\n",
    "        action = self._exploit_and_explore(actions)\n",
    "        self.actions.append(action)\n",
    "        return action\n",
    "\n",
    "    def backup_action_value(self, reward):\n",
    "        \"\"\"Backup action value for stationary problem.\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "        action = self.actions[-1]\n",
    "        self.N[action] += 1\n",
    "        self.Q[action] += 1 / self.N[action] * (reward - self.Q[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_armed_testbed(K, bandits, runs=2000, steps=1000, print_per_runs=100):\n",
    "    n_bandits = len(bandits)\n",
    "    rewards = np.zeros((n_bandits, runs, steps))\n",
    "    optimal_actions = np.zeros((n_bandits, runs, steps))\n",
    "\n",
    "    for i, bandit in enumerate(bandits):\n",
    "        for r in range(runs):\n",
    "            env = Environment(K)\n",
    "            bandit.init_action_values()\n",
    "\n",
    "            for s in range(steps):\n",
    "                # Environment and agent interact with each other.\n",
    "                action = bandit.select_action(env)\n",
    "                reward = env.step(action)\n",
    "                bandit.backup_action_value(reward)\n",
    "\n",
    "                # Store reward and optimal action indicator.\n",
    "                rewards[i, r, s] = reward\n",
    "                if action == env.optim_action:\n",
    "                    optimal_actions[i, r, s] = 1\n",
    "\n",
    "            if (r + 1) % print_per_runs == 0:\n",
    "                print('Run {} for agent {} is completed.'.format(r + 1, i))     \n",
    "\n",
    "    # Average along runs.\n",
    "    avg_rewards = rewards.mean(axis=1)\n",
    "    avg_optim_actions = optimal_actions.mean(axis=1)\n",
    "    return avg_rewards, avg_optim_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dir for notebook.\n",
    "import os\n",
    "\n",
    "images_dir = '../images/'\n",
    "if not os.path.exists(images_dir):\n",
    "    os.makedirs(images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2.1\n",
    "\n",
    "An example bandit problem from the 10-armed testbed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure2_1():\n",
    "    plt.violinplot(dataset=np.random.randn(K) + np.random.randn(200, K))\n",
    "    plt.xlabel(\"Action\")\n",
    "    plt.ylabel(\"Reward distribution\")\n",
    "    plt.hlines(y=0, xmin=0.5, xmax=10.5, linestyles='dashed')\n",
    "    plt.savefig('../images/figure2.1.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure2_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2.2\n",
    "\n",
    "Average performance of $\\epsilon$-greedy action-value methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure2_2():\n",
    "    epsilons = [0, 0.01, 0.1]\n",
    "    bandits = [MultiArmedBanditAgent(K, epsilon) for epsilon in epsilons]\n",
    "    avg_rewards, avg_optim_actions = k_armed_testbed(\n",
    "        K, bandits, runs=2000, steps=1000)\n",
    "    print('avg_rewards: {}'.format(avg_rewards))\n",
    "    print('avg_optim_actions: {}'.format(avg_optim_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 100 for agent 0 is completed.\n",
      "Run 200 for agent 0 is completed.\n",
      "Run 300 for agent 0 is completed.\n",
      "Run 400 for agent 0 is completed.\n",
      "Run 500 for agent 0 is completed.\n",
      "Run 600 for agent 0 is completed.\n",
      "Run 700 for agent 0 is completed.\n",
      "Run 800 for agent 0 is completed.\n",
      "Run 900 for agent 0 is completed.\n",
      "Run 1000 for agent 0 is completed.\n",
      "Run 1100 for agent 0 is completed.\n",
      "Run 1200 for agent 0 is completed.\n",
      "Run 1300 for agent 0 is completed.\n",
      "Run 1400 for agent 0 is completed.\n",
      "Run 1500 for agent 0 is completed.\n",
      "Run 1600 for agent 0 is completed.\n",
      "Run 1700 for agent 0 is completed.\n",
      "Run 1800 for agent 0 is completed.\n",
      "Run 1900 for agent 0 is completed.\n",
      "Run 2000 for agent 0 is completed.\n",
      "Run 100 for agent 1 is completed.\n",
      "Run 200 for agent 1 is completed.\n",
      "Run 300 for agent 1 is completed.\n",
      "Run 400 for agent 1 is completed.\n",
      "Run 500 for agent 1 is completed.\n",
      "Run 600 for agent 1 is completed.\n",
      "Run 700 for agent 1 is completed.\n",
      "Run 800 for agent 1 is completed.\n",
      "Run 900 for agent 1 is completed.\n",
      "Run 1000 for agent 1 is completed.\n",
      "Run 1100 for agent 1 is completed.\n",
      "Run 1200 for agent 1 is completed.\n",
      "Run 1300 for agent 1 is completed.\n",
      "Run 1400 for agent 1 is completed.\n",
      "Run 1500 for agent 1 is completed.\n",
      "Run 1600 for agent 1 is completed.\n",
      "Run 1700 for agent 1 is completed.\n",
      "Run 1800 for agent 1 is completed.\n",
      "Run 1900 for agent 1 is completed.\n",
      "Run 2000 for agent 1 is completed.\n",
      "Run 100 for agent 2 is completed.\n",
      "Run 200 for agent 2 is completed.\n",
      "Run 300 for agent 2 is completed.\n",
      "Run 400 for agent 2 is completed.\n",
      "Run 500 for agent 2 is completed.\n",
      "Run 600 for agent 2 is completed.\n",
      "Run 700 for agent 2 is completed.\n",
      "Run 800 for agent 2 is completed.\n",
      "Run 900 for agent 2 is completed.\n",
      "Run 1000 for agent 2 is completed.\n",
      "Run 1100 for agent 2 is completed.\n",
      "Run 1200 for agent 2 is completed.\n",
      "Run 1300 for agent 2 is completed.\n",
      "Run 1400 for agent 2 is completed.\n",
      "Run 1500 for agent 2 is completed.\n",
      "Run 1600 for agent 2 is completed.\n",
      "Run 1700 for agent 2 is completed.\n",
      "Run 1800 for agent 2 is completed.\n",
      "Run 1900 for agent 2 is completed.\n",
      "Run 2000 for agent 2 is completed.\n",
      "avg_rewards: [[-0.01113629  0.27566083  0.46050879 ...  1.08986817  1.06868712\n",
      "   1.04539263]\n",
      " [-0.01443238  0.26579202  0.45368934 ...  1.32121926  1.31229082\n",
      "   1.25791557]\n",
      " [-0.03871855  0.17950316  0.39075267 ...  1.34262219  1.35518847\n",
      "   1.38092453]]\n",
      "avg_optim_actions: [[0.1    0.144  0.179  ... 0.3795 0.3795 0.3795]\n",
      " [0.0915 0.15   0.1735 ... 0.5875 0.5905 0.587 ]\n",
      " [0.103  0.1395 0.173  ... 0.7825 0.79   0.7785]]\n"
     ]
    }
   ],
   "source": [
    "figure2_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
